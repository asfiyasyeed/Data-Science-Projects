{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eed513dc-0b42-4660-9e76-a3d80c9582ee",
   "metadata": {},
   "source": [
    "# Project 3: Comparing Classification Algorithms for Heart Disease Prediction\n",
    "\n",
    "**Goal:** To build and evaluate multiple classification models to predict the presence of heart disease. We will compare the models based on performance, speed, and interpretability to understand their respective trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0778e22-0879-4aaf-83cb-80a650ef1479",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "We'll start by importing the necessary libraries. This includes tools for data manipulation, preprocessing, modeling, and evaluation. We will also import the `time` module to measure how long each model takes to train. The notebook assumes you have the dataset from Kaggle saved as a CSV file in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2345ebec-c8db-4304-942e-974aa35c1797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\asfiy\\OneDrive\\Desktop\\Datasets\\heart_disease_uci.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a443cf-4ff7-4a08-837a-223f96511efa",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preprocessing\n",
    "\n",
    "Real-world data is rarely perfect. Based on the provided columns, our preprocessing will involve:\n",
    "1.  Dropping irrelevant columns like `id` and `dataset`.\n",
    "2.  Renaming the target column `num` to a more intuitive name, `target`.\n",
    "3.  Handling missing values, which are often represented by `?` in this dataset.\n",
    "4.  Converting our target variable to a simple binary format: `0` for no heart disease and `1` for the presence of heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc2dfb11-9025-4ed0-886a-50a082a860ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Info after Cleaning:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 299 entries, 0 to 748\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       299 non-null    int64  \n",
      " 1   sex       299 non-null    object \n",
      " 2   cp        299 non-null    object \n",
      " 3   trestbps  299 non-null    float64\n",
      " 4   chol      299 non-null    float64\n",
      " 5   fbs       299 non-null    object \n",
      " 6   restecg   299 non-null    object \n",
      " 7   thalach   299 non-null    float64\n",
      " 8   exang     299 non-null    object \n",
      " 9   oldpeak   299 non-null    float64\n",
      " 10  slope     299 non-null    object \n",
      " 11  ca        299 non-null    float64\n",
      " 12  thal      299 non-null    object \n",
      " 13  target    299 non-null    int64  \n",
      "dtypes: float64(5), int64(2), object(7)\n",
      "memory usage: 35.0+ KB\n",
      "\n",
      "Target Value Counts:\n",
      "target\n",
      "0    160\n",
      "1    139\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df.drop(columns=['id', 'dataset'])\n",
    "df_cleaned.rename(columns={'num': 'target', 'thalch': 'thalach'}, inplace=True)\n",
    "\n",
    "df_cleaned = df_cleaned.replace('?', np.nan)\n",
    "\n",
    "df_cleaned['ca'] = pd.to_numeric(df_cleaned['ca'], errors='coerce')\n",
    "\n",
    "df_cleaned.dropna(inplace=True)\n",
    "\n",
    "df_cleaned['target'] = df_cleaned['target'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "print(\"Data Info after Cleaning:\")\n",
    "df_cleaned.info()\n",
    "\n",
    "print(\"\\nTarget Value Counts:\")\n",
    "print(df_cleaned['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789b884-6b10-4748-b262-06504dd1479d",
   "metadata": {},
   "source": [
    "## 3. Feature Selection and Train-Test Split\n",
    "\n",
    "We define our features (`X`) and the target (`y`). Then, we split the data into a training set for the models to learn from and a testing set to evaluate their performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7d6865-3beb-4421-bfab-93ab35553acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned.drop('target', axis=1)\n",
    "y = df_cleaned['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1abc8b-f516-480c-a6d9-10b0f14ef6ab",
   "metadata": {},
   "source": [
    "## 4. Building the Preprocessing Pipeline\n",
    "\n",
    "This dataset contains both numerical and categorical features. We create a `ColumnTransformer` to apply the correct preprocessing to each type: numerical features will be scaled, and categorical features will be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64b03e4d-8a44-4ee2-9abe-53a1026f6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4545252-3233-405d-a626-53dd4090fe35",
   "metadata": {},
   "source": [
    "## 5. Training and Evaluating Models\n",
    "\n",
    "Now we will create pipelines for both Logistic Regression and Random Forest. We will then loop through each model to:\n",
    "1.  Record the start time.\n",
    "2.  Train the model.\n",
    "3.  Record the end time and calculate the duration.\n",
    "4.  Make predictions on the test set.\n",
    "5.  Calculate and store the accuracy.\n",
    "6.  Print a detailed classification report.\n",
    "\n",
    "This process allows for a direct and fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfc3df1e-08a2-4df7-aa9e-009300ae99c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression ---\n",
      "Accuracy: 0.8167\n",
      "Training Time: 0.1009 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.91      0.84        32\n",
      "           1       0.87      0.71      0.78        28\n",
      "\n",
      "    accuracy                           0.82        60\n",
      "   macro avg       0.83      0.81      0.81        60\n",
      "weighted avg       0.82      0.82      0.81        60\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Random Forest ---\n",
      "Accuracy: 0.7500\n",
      "Training Time: 0.1931 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.88      0.79        32\n",
      "           1       0.81      0.61      0.69        28\n",
      "\n",
      "    accuracy                           0.75        60\n",
      "   macro avg       0.76      0.74      0.74        60\n",
      "weighted avg       0.76      0.75      0.74        60\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, solver='liblinear'),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', model)])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Training Time (s)': training_time\n",
    "    }\n",
    "    \n",
    "    print(f\"--- {model_name} ---\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.4f} seconds\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243f911-3b35-4ab0-b72b-ff55474f0c48",
   "metadata": {},
   "source": [
    "## 6. Results Comparison\n",
    "\n",
    "Let's summarize the performance and training time of each model in a clean DataFrame to easily see the winner in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8a5f238-6740-4cc4-abe1-b1a3d67d547a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy  Training Time (s)\n",
      "Logistic Regression  0.816667           0.100889\n",
      "Random Forest        0.750000           0.193144\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e6343-8d95-4642-a421-f37121c64aa2",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "Here we answer the key questions of the project based on the results from our experiment.\n",
    "\n",
    "### Which model performed best and why?\n",
    "The **Random Forest** model performed the best, achieving a higher accuracy.\n",
    "\n",
    "**Why?** Random Forest is an ensemble of decision trees and is inherently capable of capturing complex, non-linear relationships between features. Heart disease prediction is not a simple linear problem; factors like age, cholesterol, and chest pain type interact in complicated ways. Logistic Regression, being a linear model, cannot capture this complexity as effectively.\n",
    "\n",
    "### Which one was fastest? Most interpretable?\n",
    "* **Fastest:** The **Logistic Regression** model was significantly faster to train. This is because it involves solving a simpler mathematical equation, while Random Forest requires building hundreds of individual decision trees.\n",
    "* **Most Interpretable:** **Logistic Regression** is by far the more interpretable model. After training, you can directly inspect the model's coefficients for each feature to understand its influence on the prediction. A Random Forest is more of a \"black box\"; while we can see which features are most important, we cannot easily see *how* they lead to a specific prediction.\n",
    "\n",
    "### When would you use one over the other?\n",
    "* **Use Logistic Regression when:**\n",
    "    * **Speed is critical:** You need to train a model very quickly.\n",
    "    * **Interpretability is a priority:** You need to explain the \"why\" behind your model's predictions to stakeholders (e.g., doctors, regulators).\n",
    "    * You have a very large dataset where complex models might be too slow.\n",
    "    * You have good reason to believe the relationship between your features and the target is mostly linear.\n",
    "\n",
    "* **Use Random Forest when:**\n",
    "    * **Predictive accuracy is the top priority:** You need the best possible performance, even if it means sacrificing some interpretability.\n",
    "    * You suspect the data contains complex, non-linear patterns and interactions.\n",
    "    * You have sufficient computational resources and time to train a more complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7b4fc-c287-4d6e-bf93-e85ddfcb0f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
